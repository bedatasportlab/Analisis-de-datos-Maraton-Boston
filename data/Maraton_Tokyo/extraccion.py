import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from config import MODO_EXTRACCION, CONFIGURACION, DELAY_ENTRE_REQUESTS

def get_runner_details(bib_number, session, base_url):
    """
    Obtiene los detalles de un corredor espec√≠fico, incluyendo sus tiempos parciales.
    """
    detail_url = f"{base_url}/detail.php"
    form_data = {'d_number': bib_number}
    
    # Headers m√°s completos para simular un navegador real
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'Referer': f"{base_url}/index.php",
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Pausa antes de cada intento
            if attempt > 0:
                wait_time = 5 * (2 ** attempt)  # Backoff exponencial: 5, 10, 20 segundos
                print(f"  ‚è≥ Reintentando en {wait_time} segundos... (intento {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
            
            response = session.post(detail_url, data=form_data, headers=headers, timeout=30)
            
            if response.status_code == 403:
                print(f"  üö´ Error 403 para BIB {bib_number} (intento {attempt + 1}/{max_retries})")
                if attempt == max_retries - 1:
                    print(f"  ‚ùå Servidor bloque√≥ BIB {bib_number} despu√©s de {max_retries} intentos")
                    return None
                continue
                
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # Debug: guardar la primera respuesta para an√°lisis
            if bib_number == '4':  # Solo para el primer corredor
                with open('runner_detail_debug.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"Debug: Guardado detalle del corredor {bib_number} en runner_detail_debug.html")

            # Extraer datos b√°sicos del corredor
            runner_data = {'BIB': bib_number}
            
            # Buscar tablas con informaci√≥n
            tables = soup.find_all('table', class_='m-item_tbl')
            print(f"Debug: Encontradas {len(tables)} tablas para BIB {bib_number}")
            
            if not tables:
                print(f"Debug: No se encontraron tablas para BIB {bib_number}")
                return None

            # Procesar la primera tabla (posici√≥n, BIB, nombre)
            if len(tables) > 0:
                first_table = tables[0]
                data_row = None
                for row in first_table.find_all('tr'):
                    cols = row.find_all('td')
                    if len(cols) == 3 and cols[1].text.strip() == str(bib_number):
                        runner_data['posicion'] = cols[0].text.strip()
                        runner_data['BIB'] = cols[1].text.strip()
                        runner_data['Nombre'] = cols[2].text.strip().replace('Ôºè', ' / ')
                        break

            # Procesar la segunda tabla (informaci√≥n detallada)
            if len(tables) > 1:
                info_table = tables[1]
                for row in info_table.find_all('tr'):
                    cols = row.find_all('td')
                    th_element = row.find('th')
                    
                    if th_element and len(cols) >= 1:
                        th = th_element.text.strip()
                        td = cols[0].text.strip()
                        
                        # Mapear campos espec√≠ficos
                        if 'ÂèÇÂä†Á®ÆÁõÆ' in th or 'Race Category' in th:
                            runner_data['categoria'] = td
                            # Filtro: Solo procesar Marathon Men y Marathon Women
                            if 'Wheelchair' in td or 'Junior' in td or 'Ëªä„ÅÑ„Åô' in td or '„Ç∏„É•„Éã„Ç¢' in td or 'Áü•ÁöÑÈöú„Åå„ÅÑ' in td or 'Ë¶ñË¶öÈöú„Åå„ÅÑ' in td or 'ÁßªÊ§çËÄÖ' in td:
                                print(f"Debug: Saltando corredor BIB {bib_number} - Categor√≠a: {td}")
                                return None
                        elif 'Âπ¥ÈΩ¢' in th or 'Age' in th:
                            runner_data['Edad'] = td
                        elif 'ÊÄßÂà•' in th or 'Sex' in th:
                            runner_data['Genero'] = td
                        elif 'ÂõΩÁ±ç' in th or 'Nationality' in th:
                            runner_data['Nacionalidad'] = td
                        elif '„Çø„Ç§„É†(„Éç„ÉÉ„Éà)' in th or 'Time (net)' in th:
                            runner_data['tiempo_neto'] = td
                        elif '„Çø„Ç§„É†(„Ç∞„É≠„Çπ)' in th or 'Time (gross)' in th:
                            runner_data['tiempo_oficial'] = td

            # Procesar tablas de tiempos parciales (tabla 3 espec√≠ficamente)
            if len(tables) > 2:
                splits_table = tables[2]  # La tercera tabla contiene los splits
                print(f"Debug: Procesando tabla de splits (tabla 3)")
                
                # Extraer el HTML completo de la tabla para an√°lisis
                table_html = str(splits_table)
                
                # Usar regex para extraer los tiempos de manera m√°s precisa
                import re
                
                # Patr√≥n para 5km (m√°s flexible con espacios en blanco)
                match_5km = re.search(r'<td[^>]*class="taR"[^>]*>5km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_5km and not runner_data.get('parcial_5km'):
                    runner_data['parcial_5km'] = match_5km.group(1)
                    print(f"Debug: 5km = {match_5km.group(1)}")
                
                # Patr√≥n para 10km
                match_10km = re.search(r'<td[^>]*class="taR"[^>]*>10km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_10km and not runner_data.get('parcial_10km'):
                    runner_data['parcial_10km'] = match_10km.group(1)
                    print(f"Debug: 10km = {match_10km.group(1)}")
                
                # Patr√≥n para 15km
                match_15km = re.search(r'<td[^>]*class="taR"[^>]*>15km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_15km and not runner_data.get('parcial_15km'):
                    runner_data['parcial_15km'] = match_15km.group(1)
                    print(f"Debug: 15km = {match_15km.group(1)}")
                
                # Patr√≥n para 20km
                match_20km = re.search(r'<td[^>]*class="taR"[^>]*>20km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_20km and not runner_data.get('parcial_20km'):
                    runner_data['parcial_20km'] = match_20km.group(1)
                    print(f"Debug: 20km = {match_20km.group(1)}")
                
                # Patr√≥n para Halfway Point
                match_halfway = re.search(r'<td[^>]*class="taR"[^>]*>‰∏≠ÈñìÁÇπÔºèHalfway Point</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_halfway and not runner_data.get('medio_maraton'):
                    runner_data['medio_maraton'] = match_halfway.group(1)
                    print(f"Debug: Halfway = {match_halfway.group(1)}")
                
                # Patr√≥n para 25km
                match_25km = re.search(r'<td[^>]*class="taR"[^>]*>25km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_25km and not runner_data.get('parcial_25km'):
                    runner_data['parcial_25km'] = match_25km.group(1)
                    print(f"Debug: 25km = {match_25km.group(1)}")
                
                # Patr√≥n para 30km
                match_30km = re.search(r'<td[^>]*class="taR"[^>]*>30km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_30km and not runner_data.get('parcial_30km'):
                    runner_data['parcial_30km'] = match_30km.group(1)
                    print(f"Debug: 30km = {match_30km.group(1)}")
                
                # Patr√≥n para 35km
                match_35km = re.search(r'<td[^>]*class="taR"[^>]*>35km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_35km and not runner_data.get('parcial_35km'):
                    runner_data['parcial_35km'] = match_35km.group(1)
                    print(f"Debug: 35km = {match_35km.group(1)}")
                
                # Patr√≥n para 40km
                match_40km = re.search(r'<td[^>]*class="taR"[^>]*>40km</td>\s*<td[^>]*class="taC"[^>]*>([0-9:]+)</td>', table_html)
                if match_40km and not runner_data.get('parcial_40km'):
                    runner_data['parcial_40km'] = match_40km.group(1)
                    print(f"Debug: 40km = {match_40km.group(1)}")

            print(f"Debug: Datos extra√≠dos para BIB {bib_number}: {list(runner_data.keys())}")
            return runner_data
            
        except requests.RequestException as e:
            print(f"  ‚ö†Ô∏è  Error de conexi√≥n para BIB {bib_number} (intento {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"  ‚ùå Fall√≥ BIB {bib_number} despu√©s de {max_retries} intentos")
                return None
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error inesperado para BIB {bib_number} (intento {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"  ‚ùå Fall√≥ BIB {bib_number} despu√©s de {max_retries} intentos")
                return None
    
    return None

def main():
    base_url = "https://www.marathon.tokyo/2024/result"
    index_url = f"{base_url}/index.php"
    session = requests.Session()
    all_runners_data = []
    total_athletes_processed = 0  # Contador global de atletas procesados
    start_time = time.time()  # Tiempo de inicio para c√°lculos de velocidad

    # Obtener configuraci√≥n del modo seleccionado
    config = CONFIGURACION[MODO_EXTRACCION]
    print(f"üéØ {config['descripcion']}")

    # Headers para simular un navegador
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': base_url,
        'Origin': 'https://www.marathon.tokyo',
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    try:
        # 1. Primero hacer GET para obtener la p√°gina inicial y establecer cookies
        print("Obteniendo p√°gina inicial...")
        initial_response = session.get(base_url, headers=headers)
        initial_response.raise_for_status()
        
        # 2. Hacer POST a index.php (como indica el action del form)
        print("Enviando formulario de b√∫squeda...")
        form_data = {
            'category': '',
            'number': '',
            'name': '',
            'age': '',
            'sex[]': '',
            'country': '',
            'prefecture': '',
            'sort_key': 'place',
            'sort_asc': '1',
            'page': '1',
            'd_number': ''
        }
        
        response = session.post(index_url, data=form_data, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extraer el n√∫mero total de p√°ginas (con manejo de errores)
        pager_div = soup.find('div', class_='fnav pager')
        if not pager_div:
            print("Error: No se encontr√≥ la secci√≥n de paginaci√≥n.")
            print("Intentando buscar elementos de resultados en la p√°gina...")
            
            # Verificar si hay una tabla de resultados
            results_table = soup.find('table', class_='m-item_tbl mb10')
            if results_table:
                print("Se encontr√≥ una tabla de resultados. Procesando...")
                # Extraer informaci√≥n de paginaci√≥n del texto
                page_info = soup.find('p', class_='taR')
                if page_info and 'Ôºè' in page_info.text:
                    total_runners = int(page_info.text.split('Ôºè')[-1])
                    total_pages = (total_runners // 50) + 1
                    print(f"Se encontraron {total_runners} corredores en {total_pages} p√°ginas.")
                else:
                    print("No se pudo determinar el n√∫mero total de p√°ginas. Procesando solo la primera p√°gina.")
                    total_pages = 1
            else:
                print("No se encontraron resultados. Guardando el HTML en 'error_page.html'")
                with open('error_page.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                return
        else:
            pager_text = pager_div.find('p', class_='taR').text
            total_runners = int(pager_text.split('Ôºè')[-1])
            total_pages = (total_runners // 50) + 1
            print(f"Se encontraron {total_runners} corredores en {total_pages} p√°ginas.")
            
            # Determinar p√°ginas a procesar seg√∫n configuraci√≥n
            if config['max_pages'] is None:
                pages_to_process = total_pages
                print(f"üöÄ MODO COMPLETO: Procesando todas las {total_pages} p√°ginas ({total_runners} corredores).")
                print(f"‚è±Ô∏è  Tiempo estimado: {total_runners * 1.5 / 3600:.1f} horas")
            else:
                pages_to_process = min(config['max_pages'], total_pages)
                estimated_runners = pages_to_process * (config['max_runners_per_page'] or 50)
                print(f"üß™ MODO PRUEBA: Procesando {pages_to_process} de {total_pages} p√°ginas (~{estimated_runners} corredores m√°ximo).")
            
            print(f"üìä Progreso se mostrar√° cada atleta consultado...")

        # 2. Iterar a trav√©s de las p√°ginas de resultados
        for page_num in range(1, pages_to_process + 1):
            print(f"\nProcesando p√°gina de resultados {page_num}/{pages_to_process}...")
            
            # Para la primera p√°gina, usar la respuesta que ya tenemos
            if page_num == 1:
                page_soup = soup
            else:
                form_data = {
                    'category': '',
                    'number': '',
                    'name': '',
                    'age': '',
                    'sex[]': '',
                    'country': '',
                    'prefecture': '',
                    'sort_key': 'place',
                    'sort_asc': '1',
                    'page': str(page_num),
                    'd_number': ''
                }
                page_response = session.post(index_url, data=form_data, headers=headers)
                page_soup = BeautifulSoup(page_response.text, 'html.parser')
            
            # Extraer los BIB numbers de la tabla de la p√°gina actual
            bib_numbers = []
            results_table = page_soup.find('table', class_='m-item_tbl mb10')
            if results_table:
                for row in results_table.find_all('tr')[1:]:
                    link = row.find('a', href=lambda href: href and 'javascript:detail' in href)
                    if link:
                        match = re.search(r"javascript:detail\('(\d+)'\);", link['href'])
                        if match:
                            bib_numbers.append(match.group(1))
            
            # Aplicar l√≠mite de corredores por p√°gina si est√° configurado
            if config['max_runners_per_page'] is not None:
                bib_numbers = bib_numbers[:config['max_runners_per_page']]
            
            print(f"Encontrados {len(bib_numbers)} corredores en esta p√°gina. Obteniendo detalles...")

            # 3. Para cada BIB, obtener los detalles completos
            for i, bib in enumerate(bib_numbers):
                total_athletes_processed += 1
                
                # Calcular porcentaje din√°micamente
                if config['max_pages'] is None:
                    # Modo completo: usar total real de corredores
                    percentage = (total_athletes_processed / total_runners) * 100
                    total_text = str(total_runners)
                else:
                    # Modo prueba: calcular estimado
                    estimated_total = pages_to_process * (config['max_runners_per_page'] or 50)
                    percentage = (total_athletes_processed / estimated_total) * 100 if estimated_total > 0 else 0
                    total_text = f"~{estimated_total}"
                
                print(f"Consultando atleta {total_athletes_processed} de {total_text} ({percentage:.2f}%) - P√°gina {page_num}/{pages_to_process} - Corredor {i+1}/{len(bib_numbers)} - BIB: {bib}")
                details = get_runner_details(bib, session, base_url)
                if details:
                    print(f"  ‚úÖ Datos extra√≠dos para BIB: {details.get('BIB', bib)}")
                    all_runners_data.append(details)
                else:
                    print(f"  ‚ùå No se pudieron extraer datos para BIB: {bib}")
                
                # Resumen de progreso (cada 50 en prueba, cada 100 en completo)
                progress_interval = 50 if config['max_pages'] is not None else 100
                if total_athletes_processed % progress_interval == 0:
                    valid_runners = len(all_runners_data)
                    elapsed_time = time.time() - start_time
                    rate = total_athletes_processed / elapsed_time if elapsed_time > 0 else 0
                    
                    print(f"\nüìà RESUMEN PROGRESO:")
                    print(f"   üîç Atletas consultados: {total_athletes_processed}")
                    print(f"   ‚úÖ Datos v√°lidos extra√≠dos: {valid_runners}")
                    print(f"   üìä Porcentaje completado: {percentage:.2f}%")
                    print(f"   ‚ö° Velocidad: {rate:.2f} atletas/segundo")
                    print(f"   ‚è±Ô∏è  Tiempo transcurrido: {elapsed_time/3600:.2f} horas")
                    
                    # Solo mostrar ETA en modo completo
                    if config['max_pages'] is None:
                        remaining_athletes = total_runners - total_athletes_processed
                        eta_seconds = remaining_athletes / rate if rate > 0 else 0
                        eta_hours = eta_seconds / 3600
                        print(f"   üéØ Tiempo estimado restante: {eta_hours:.2f} horas")
                    print()
                
                time.sleep(DELAY_ENTRE_REQUESTS) # Pausa configurable

    except requests.RequestException as e:
        print(f"Error al hacer la petici√≥n principal: {e}")
    except Exception as e:
        print(f"Ocurri√≥ un error inesperado: {e}")

    # 4. Guardar todos los datos en un archivo CSV
    if all_runners_data:
        total_time = time.time() - start_time
        mode_text = "PRUEBA" if config['max_pages'] is not None else "COMPLETA"
        
        print(f"\nüéØ EXTRACCI√ìN {mode_text} COMPLETADA")
        print(f"üìä Atletas consultados: {total_athletes_processed}")
        print(f"‚úÖ Datos v√°lidos extra√≠dos: {len(all_runners_data)}")
        print(f"‚ùå Atletas descartados (wheelchair/junior): {total_athletes_processed - len(all_runners_data)}")
        print(f"‚è±Ô∏è  Tiempo total: {total_time/3600:.2f} horas")
        print(f"‚ö° Velocidad promedio: {total_athletes_processed/total_time:.2f} atletas/segundo")
        
        # Nombre de archivo din√°mico seg√∫n el modo
        if config['max_pages'] is not None:
            filename = f'marathon_tokyo_results_2024_{MODO_EXTRACCION}.csv'
        else:
            filename = 'marathon_tokyo_results_2024_completo.csv'
            
        print(f"üìÅ Guardando todos los datos en '{filename}'...")
        df = pd.DataFrame(all_runners_data)
        
        column_order = ["BIB", "Nombre", "Nacionalidad", "Genero", "Edad", "tiempo_oficial",
                        "parcial_5km", "parcial_10km", "parcial_15km", "parcial_20km", 
                        "medio_maraton", "parcial_25km", "parcial_30km", "parcial_35km", "parcial_40km"]
        
        for col in column_order:
            if col not in df.columns:
                df[col] = None
        
        df = df.reindex(columns=column_order)
        
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"¬°Extracci√≥n {mode_text.lower()} completada con √©xito!")
        print(f"üìã Archivo guardado: {filename}")
    else:
        print("No se pudo extraer ning√∫n dato. Revisa el script o la p√°gina web.")

if __name__ == "__main__":
    main()

